#!/usr/bin/env python3
"""
HuggingFaceæ•°æ®é›†æµå¼è§£æå™¨ - C++æ¥å£ç‰ˆæœ¬
æä¾›ä¸C++ä»£ç äº¤äº’çš„API
"""

import sys
import json
import argparse
from datasets import load_dataset
import traceback

def stream_huggingface_dataset(dataset_name, subset, split, max_entries, category, output_file):
    """
    æµå¼è§£æHuggingFaceæ•°æ®é›†å¹¶è¾“å‡ºåˆ°æ–‡ä»¶
    
    Args:
        dataset_name: æ•°æ®é›†åç§°
        subset: å­é›†åç§°
        split: æ•°æ®åˆ†å‰²
        max_entries: æœ€å¤§æ¡ç›®æ•°
        category: ç±»åˆ«
        output_file: è¾“å‡ºæ–‡ä»¶è·¯å¾„
    """
    try:
        print(f"ğŸ” åŠ è½½æ•°æ®é›†: {dataset_name}")
        if subset:
            dataset = load_dataset(dataset_name, subset, split=split, streaming=True)
        else:
            dataset = load_dataset(dataset_name, split=split, streaming=True)
        
        entries = []
        count = 0
        
        print(f"ğŸ“¥ æµå¼è·å–æ•°æ®...")
        for item in dataset:
            if count >= max_entries:
                break
            
            # å°†æ•°æ®è½¬æ¢ä¸ºKnowledgeEntryæ ¼å¼
            entry = {
                "title": f"HFæ•°æ®é›†æ¡ç›® {count+1}",
                "content": str(item),
                "category": category,
                "source": f"huggingface://{dataset_name}/{subset}/{split}",
                "relevance_score": 0.8,
                "tags": []
            }
            
            entries.append(entry)
            count += 1
            
            if count % 10 == 0:
                print(f"   å·²å¤„ç† {count} ä¸ªæ¡ç›®")
        
        # ä¿å­˜åˆ°JSONæ–‡ä»¶
        with open(output_file, 'w', encoding='utf-8') as f:
            json.dump(entries, f, ensure_ascii=False, indent=2)
        
        print(f"âœ… æˆåŠŸå¤„ç† {count} ä¸ªæ¡ç›®ï¼Œç»“æœä¿å­˜åˆ° {output_file}")
        return True
        
    except Exception as e:
        print(f"âŒ å¤„ç†æ•°æ®é›†æ—¶å‡ºé”™: {e}")
        traceback.print_exc()
        return False

def query_huggingface_dataset(dataset_name, subset, query, max_results, category, output_file):
    """
    æŸ¥è¯¢HuggingFaceæ•°æ®é›†å¹¶è¾“å‡ºç»“æœ
    
    Args:
        dataset_name: æ•°æ®é›†åç§°
        subset: å­é›†åç§°
        query: æŸ¥è¯¢å…³é”®è¯
        max_results: æœ€å¤§ç»“æœæ•°
        category: ç±»åˆ«
        output_file: è¾“å‡ºæ–‡ä»¶è·¯å¾„
    """
    try:
        print(f"ğŸ” æŸ¥è¯¢æ•°æ®é›†: {dataset_name}")
        if subset:
            dataset = load_dataset(dataset_name, subset, split='train', streaming=True)
        else:
            dataset = load_dataset(dataset_name, split='train', streaming=True)
        
        results = []
        count = 0
        
        print(f"ğŸ“¥ æŸ¥è¯¢ç›¸å…³æ¡ç›®...")
        for item in dataset:
            if count >= max_results:
                break
            
            # ç®€å•çš„æ–‡æœ¬åŒ¹é…
            item_text = str(item).lower()
            if query.lower() in item_text:
                entry = {
                    "title": f"æŸ¥è¯¢ç»“æœ {count+1}: {query}",
                    "content": str(item),
                    "category": category,
                    "source": f"huggingface-query://{dataset_name}/{subset}",
                    "relevance_score": 0.7 + (0.2 * count / max_results),
                    "tags": []
                }
                
                results.append(entry)
                count += 1
                
                if count % 5 == 0:
                    print(f"   å·²æ‰¾åˆ° {count} ä¸ªåŒ¹é…ç»“æœ")
        
        # ä¿å­˜åˆ°JSONæ–‡ä»¶
        with open(output_file, 'w', encoding='utf-8') as f:
            json.dump(results, f, ensure_ascii=False, indent=2)
        
        print(f"âœ… æˆåŠŸæ‰¾åˆ° {count} ä¸ªåŒ¹é…ç»“æœï¼Œç»“æœä¿å­˜åˆ° {output_file}")
        return True
        
    except Exception as e:
        print(f"âŒ æŸ¥è¯¢æ•°æ®é›†æ—¶å‡ºé”™: {e}")
        traceback.print_exc()
        return False

def main():
    """ä¸»å‡½æ•° - å‘½ä»¤è¡Œæ¥å£"""
    parser = argparse.ArgumentParser(description='HuggingFaceæ•°æ®é›†æµå¼è§£æå™¨')
    parser.add_argument('action', choices=['stream', 'query'], help='æ“ä½œç±»å‹')
    parser.add_argument('--dataset', required=True, help='æ•°æ®é›†åç§°')
    parser.add_argument('--subset', default='', help='æ•°æ®é›†å­é›†')
    parser.add_argument('--split', default='train', help='æ•°æ®åˆ†å‰²')
    parser.add_argument('--max-entries', type=int, default=100, help='æœ€å¤§æ¡ç›®æ•°')
    parser.add_argument('--query', default='', help='æŸ¥è¯¢å…³é”®è¯')
    parser.add_argument('--category', default='huggingface', help='ç±»åˆ«')
    parser.add_argument('--output', required=True, help='è¾“å‡ºæ–‡ä»¶è·¯å¾„')
    
    args = parser.parse_args()
    
    print("ğŸŒŠ HuggingFaceæ•°æ®é›†æµå¼è§£æå™¨")
    print("=" * 40)
    
    if args.action == 'stream':
        success = stream_huggingface_dataset(
            args.dataset, args.subset, args.split, 
            args.max_entries, args.category, args.output
        )
    elif args.action == 'query':
        if not args.query:
            print("âŒ æŸ¥è¯¢æ“ä½œéœ€è¦æä¾›æŸ¥è¯¢å…³é”®è¯")
            return False
        success = query_huggingface_dataset(
            args.dataset, args.subset, args.query,
            args.max_entries, args.category, args.output
        )
    else:
        print("âŒ æœªçŸ¥æ“ä½œç±»å‹")
        return False
    
    if success:
        print("\nğŸ‰ æ“ä½œæˆåŠŸå®Œæˆï¼")
        return True
    else:
        print("\nâŒ æ“ä½œå¤±è´¥ï¼")
        return False

if __name__ == "__main__":
    success = main()
    sys.exit(0 if success else 1)
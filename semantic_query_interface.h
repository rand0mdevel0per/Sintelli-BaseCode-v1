// è¯­ä¹‰æŸ¥è¯¢æ¥å£å°è£…
// æä¾›ç¼“å­˜ã€ä¸Šä¸‹æ–‡æ„ŸçŸ¥ã€é¢‘ç‡é™åˆ¶ç­‰é«˜çº§åŠŸèƒ½

#ifndef SEMANTIC_QUERY_INTERFACE_H
#define SEMANTIC_QUERY_INTERFACE_H

#include <string>
#include <vector>
#include <unordered_map>
#include <queue>
#include <chrono>
#include <functional>
#include <mutex>
#include "semantic_matcher.h"

// ç¼“å­˜æ¡ç›®ç»“æ„
struct CachedQueryResult {
    std::string query_text;
    FeatureVector<float> query_feature;
    std::vector<std::pair<LogicDescriptor, double>> results;
    std::chrono::steady_clock::time_point timestamp;
    uint32_t hit_count = 0;
    double confidence = 0.0; // æŸ¥è¯¢ç½®ä¿¡åº¦
};

// æŸ¥è¯¢ä¼˜å…ˆçº§ç»“æ„
struct PriorityQuery {
    std::string query;
    double confidence;        // åŸºäºç¥ç»å…ƒæ¿€æ´»åº¦çš„ç½®ä¿¡åº¦
    std::chrono::steady_clock::time_point timestamp;
    
    bool operator<(const PriorityQuery& other) const {
        // é«˜ç½®ä¿¡åº¦ä¼˜å…ˆ
        return confidence < other.confidence;
    }
};

// è¯­ä¹‰æŸ¥è¯¢ç¼“å­˜ç±»
class SemanticQueryCache {
private:
    std::unordered_map<std::string, CachedQueryResult> cache;
    size_t max_cache_size;
    std::chrono::seconds cache_ttl;
    mutable std::mutex cache_mutex;
    
public:
    SemanticQueryCache(size_t max_size = 1000, std::chrono::seconds ttl = std::chrono::seconds(300))
        : max_cache_size(max_size), cache_ttl(ttl) {}
    
    // å°è¯•ä»ç¼“å­˜è·å–ç»“æœ
    bool tryGetFromCache(const std::string& query, 
                        std::vector<std::pair<LogicDescriptor, double>>& results,
                        double& confidence) {
        std::lock_guard<std::mutex> lock(cache_mutex);
        
        auto hash = std::to_string(std::hash<std::string>{}(query));
        auto it = cache.find(hash);
        
        if (it != cache.end()) {
            auto now = std::chrono::steady_clock::now();
            if (now - it->second.timestamp < cache_ttl) {
                it->second.hit_count++;
                results = it->second.results;
                confidence = it->second.confidence;
                return true;
            } else {
                cache.erase(it);
            }
        }
        return false;
    }
    
    // æ·»åŠ åˆ°ç¼“å­˜
    void addToCache(const std::string& query, 
                   const FeatureVector<float>& feature,
                   const std::vector<std::pair<LogicDescriptor, double>>& results,
                   double confidence = 0.0) {
        std::lock_guard<std::mutex> lock(cache_mutex);
        
        auto hash = std::to_string(std::hash<std::string>{}(query));
        
        // ç¼“å­˜æ·˜æ±°ç­–ç•¥
        if (cache.size() >= max_cache_size) {
            auto oldest = cache.begin();
            for (auto it = cache.begin(); it != cache.end(); ++it) {
                if (it->second.hit_count < oldest->second.hit_count) {
                    oldest = it;
                }
            }
            cache.erase(oldest);
        }
        
        cache[hash] = {query, feature, results, std::chrono::steady_clock::now(), 0, confidence};
    }
    
    // è·å–ç¼“å­˜ç»Ÿè®¡
    struct CacheStats {
        size_t total_entries;
        size_t total_hits;
        double hit_rate;
    };
    
    CacheStats getStats() const {
        std::lock_guard<std::mutex> lock(cache_mutex);
        
        CacheStats stats{};
        stats.total_entries = cache.size();
        
        size_t total_hits = 0;
        for (const auto& entry : cache) {
            total_hits += entry.second.hit_count;
        }
        stats.total_hits = total_hits;
        stats.hit_rate = cache.empty() ? 0.0 : static_cast<double>(total_hits) / cache.size();
        
        return stats;
    }
    
    // æ¸…é™¤ç¼“å­˜
    void clearCache() {
        std::lock_guard<std::mutex> lock(cache_mutex);
        cache.clear();
    }
};

// ä¸Šä¸‹æ–‡æ„ŸçŸ¥æŸ¥è¯¢å¤„ç†å™¨
class ContextAwareQueryProcessor {
private:
    SemanticQueryCache& cache;
    std::vector<std::string> recent_queries;
    size_t context_window;
    mutable std::mutex context_mutex;
    
public:
    ContextAwareQueryProcessor(SemanticQueryCache& query_cache, size_t window_size = 10)
        : cache(query_cache), context_window(window_size) {}
    
    // å¸¦ä¸Šä¸‹æ–‡çš„æŸ¥è¯¢å¤„ç†
    std::vector<std::pair<LogicDescriptor, double>> 
    processWithContext(const std::string& current_query,
                      LogicSemanticMatcher& matcher,
                      double neuron_confidence = 0.0,
                      double similarity_threshold = 0.3) {
        
        // 1. å…ˆå°è¯•ç¼“å­˜å‘½ä¸­
        std::vector<std::pair<LogicDescriptor, double>> results;
        double cached_confidence;
        if (cache.tryGetFromCache(current_query, results, cached_confidence)) {
            std::cout << "ğŸ¯ ç¼“å­˜å‘½ä¸­! æŸ¥è¯¢: \"" << current_query 
                      << "\" (ç½®ä¿¡åº¦: " << cached_confidence << ")" << std::endl;
            return results;
        }
        
        // 2. åŸºäºä¸Šä¸‹æ–‡çš„æŸ¥è¯¢æ‰©å±•
        std::string expanded_query = expandQueryWithContext(current_query);
        
        // 3. æ‰§è¡Œè¯­ä¹‰æŸ¥è¯¢
        results = matcher.findMatchingLogics(expanded_query, 5, similarity_threshold);
        
        // 4. ç¼“å­˜ç»“æœ
        auto feature = matcher.getFeatureExtractor()->extractTextFeature(current_query);
        cache.addToCache(current_query, feature, results, neuron_confidence);
        
        // 5. æ›´æ–°ä¸Šä¸‹æ–‡
        updateContext(current_query);
        
        std::cout << "ğŸ” æ–°æŸ¥è¯¢: \"" << current_query 
                  << "\" -> æ‰¾åˆ° " << results.size() << " ä¸ªåŒ¹é…Logic" << std::endl;
        
        return results;
    }
    
    // è·å–ä¸Šä¸‹æ–‡
    std::vector<std::string> getContext() const {
        std::lock_guard<std::mutex> lock(context_mutex);
        return recent_queries;
    }
    
    // æ¸…é™¤ä¸Šä¸‹æ–‡
    void clearContext() {
        std::lock_guard<std::mutex> lock(context_mutex);
        recent_queries.clear();
    }
    
private:
    std::string expandQueryWithContext(const std::string& query) {
        std::lock_guard<std::mutex> lock(context_mutex);
        
        if (recent_queries.empty()) return query;
        
        // åˆå¹¶æœ€è¿‘æŸ¥è¯¢ä½œä¸ºä¸Šä¸‹æ–‡ï¼ˆå–æœ€åå‡ ä¸ªï¼‰
        std::string context_query = query;
        size_t start_idx = recent_queries.size() > 3 ? recent_queries.size() - 3 : 0;
        
        for (size_t i = start_idx; i < recent_queries.size(); ++i) {
            context_query += " " + recent_queries[i];
        }
        return context_query;
    }
    
    void updateContext(const std::string& query) {
        std::lock_guard<std::mutex> lock(context_mutex);
        
        recent_queries.push_back(query);
        if (recent_queries.size() > context_window) {
            recent_queries.erase(recent_queries.begin());
        }
    }
};

// è‡ªé€‚åº”é‡‡æ ·æ§åˆ¶å™¨
class AdaptiveSamplingController {
private:
    double intensity_threshold;
    std::chrono::milliseconds base_interval;
    std::chrono::milliseconds current_interval;
    std::chrono::steady_clock::time_point last_sample_time;
    mutable std::mutex sampling_mutex;
    
public:
    AdaptiveSamplingController(double threshold = 0.7, 
                             std::chrono::milliseconds interval = std::chrono::milliseconds(100))
        : intensity_threshold(threshold), base_interval(interval), current_interval(interval) {}
    
    // åˆ¤æ–­æ˜¯å¦åº”è¯¥é‡‡æ ·
    bool shouldSample(double neuron_activity, const std::string& current_content) {
        std::lock_guard<std::mutex> lock(sampling_mutex);
        
        auto now = std::chrono::steady_clock::now();
        
        // æ£€æŸ¥æ—¶é—´é—´éš”
        if (now - last_sample_time < current_interval) {
            return false;
        }
        
        // é«˜æ¿€æ´»åº¦ => æ›´é¢‘ç¹é‡‡æ ·
        if (neuron_activity > intensity_threshold) {
            current_interval = std::chrono::milliseconds(50); // 50ms
        } else {
            current_interval = std::chrono::milliseconds(500); // 500ms
        }
        
        // æ£€æŸ¥å†…å®¹æ˜¯å¦æœ‰æ„ä¹‰
        bool meaningful = isMeaningfulContent(current_content);
        
        if (meaningful) {
            last_sample_time = now;
        }
        
        return meaningful;
    }
    
    // è·å–å½“å‰é‡‡æ ·é—´éš”
    std::chrono::milliseconds getCurrentInterval() const {
        std::lock_guard<std::mutex> lock(sampling_mutex);
        return current_interval;
    }
    
    // è®¾ç½®é‡‡æ ·å‚æ•°
    void setSamplingParameters(double threshold, std::chrono::milliseconds interval) {
        std::lock_guard<std::mutex> lock(sampling_mutex);
        intensity_threshold = threshold;
        base_interval = interval;
        current_interval = interval;
    }
    
    // é‡ç½®é‡‡æ ·çŠ¶æ€
    void reset() {
        std::lock_guard<std::mutex> lock(sampling_mutex);
        current_interval = base_interval;
        last_sample_time = std::chrono::steady_clock::time_point{};
    }
    
private:
    bool isMeaningfulContent(const std::string& content) {
        // ç®€å•çš„æœ‰æ„ä¹‰å†…å®¹æ£€æµ‹
        static std::string last_content;
        
        // å†…å®¹é•¿åº¦æ£€æŸ¥
        if (content.length() < 3) return false;
        
        // å†…å®¹å˜åŒ–æ£€æŸ¥
        if (content == last_content) return false;
        
        last_content = content;
        return true;
    }
};

// æŸ¥è¯¢ä¼˜å…ˆçº§ç®¡ç†å™¨
class QueryPriorityManager {
private:
    std::priority_queue<PriorityQuery> query_queue;
    size_t max_queue_size;
    mutable std::mutex queue_mutex;
    
public:
    QueryPriorityManager(size_t max_size = 100) : max_queue_size(max_size) {}
    
    // æ·»åŠ æŸ¥è¯¢åˆ°ä¼˜å…ˆçº§é˜Ÿåˆ—
    void addQuery(const std::string& query, double confidence) {
        std::lock_guard<std::mutex> lock(queue_mutex);
        
        PriorityQuery pq{query, confidence, std::chrono::steady_clock::now()};
        query_queue.push(pq);
        
        // é™åˆ¶é˜Ÿåˆ—å¤§å°
        if (query_queue.size() > max_queue_size) {
            // ç§»é™¤ç½®ä¿¡åº¦æœ€ä½çš„æŸ¥è¯¢
            std::priority_queue<PriorityQuery> temp;
            while (query_queue.size() > max_queue_size - 1) {
                temp.push(query_queue.top());
                query_queue.pop();
            }
            query_queue = std::move(temp);
        }
    }
    
    // è·å–æœ€é«˜ä¼˜å…ˆçº§çš„æŸ¥è¯¢
    bool getNextQuery(std::string& query, double& confidence) {
        std::lock_guard<std::mutex> lock(queue_mutex);
        
        if (query_queue.empty()) return false;
        
        auto top_query = query_queue.top();
        query_queue.pop();
        
        query = top_query.query;
        confidence = top_query.confidence;
        return true;
    }
    
    // è·å–é˜Ÿåˆ—å¤§å°
    size_t getQueueSize() const {
        std::lock_guard<std::mutex> lock(queue_mutex);
        return query_queue.size();
    }
    
    // æ¸…ç©ºé˜Ÿåˆ—
    void clearQueue() {
        std::lock_guard<std::mutex> lock(queue_mutex);
        while (!query_queue.empty()) {
            query_queue.pop();
        }
    }
};

// ä¸»æŸ¥è¯¢æ¥å£ç±»
class SemanticQueryInterface {
private:
    SemanticQueryCache cache;
    ContextAwareQueryProcessor context_processor;
    AdaptiveSamplingController sampling_controller;
    QueryPriorityManager priority_manager;
    LogicSemanticMatcher& logic_matcher;
    
public:
    SemanticQueryInterface(LogicSemanticMatcher& matcher,
                          size_t cache_size = 1000,
                          size_t context_window = 10,
                          double sampling_threshold = 0.7,
                          std::chrono::milliseconds sampling_interval = std::chrono::milliseconds(100))
        : cache(cache_size),
          context_processor(cache, context_window),
          sampling_controller(sampling_threshold, sampling_interval),
          logic_matcher(matcher) {}
    
    // ä¸»æŸ¥è¯¢æ¥å£
    std::vector<std::pair<LogicDescriptor, double>> 
    query(const std::string& query_text, 
          double neuron_confidence = 0.0,
          double similarity_threshold = 0.3,
          bool use_context = true,
          bool use_cache = true) {
        
        if (!use_cache) {
            // ç›´æ¥æŸ¥è¯¢
            return logic_matcher.findMatchingLogics(query_text, 5, similarity_threshold);
        }
        
        if (use_context) {
            return context_processor.processWithContext(query_text, logic_matcher, 
                                                       neuron_confidence, similarity_threshold);
        } else {
            // ä»…ä½¿ç”¨ç¼“å­˜
            std::vector<std::pair<LogicDescriptor, double>> results;
            double confidence;
            if (cache.tryGetFromCache(query_text, results, confidence)) {
                return results;
            }
            
            // ç¼“å­˜æœªå‘½ä¸­ï¼Œæ‰§è¡ŒæŸ¥è¯¢å¹¶ç¼“å­˜
            results = logic_matcher.findMatchingLogics(query_text, 5, similarity_threshold);
            auto feature = logic_matcher.getFeatureExtractor()->extractTextFeature(query_text);
            cache.addToCache(query_text, feature, results, neuron_confidence);
            return results;
        }
    }
    
    // å¼‚æ­¥æŸ¥è¯¢ï¼ˆæ·»åŠ åˆ°ä¼˜å…ˆçº§é˜Ÿåˆ—ï¼‰
    void asyncQuery(const std::string& query_text, double confidence = 0.0) {
        priority_manager.addQuery(query_text, confidence);
    }
    
    // å¤„ç†ä¼˜å…ˆçº§é˜Ÿåˆ—ä¸­çš„æŸ¥è¯¢
    void processPriorityQueries(double similarity_threshold = 0.3) {
        std::string query;
        double confidence;
        
        while (priority_manager.getNextQuery(query, confidence)) {
            auto results = this->query(query, confidence, similarity_threshold, true, true);
            
            // å¤„ç†ç»“æœï¼ˆå¯ä»¥åœ¨è¿™é‡Œæ·»åŠ å›è°ƒæˆ–æ³¨å…¥é€»è¾‘ï¼‰
            if (!results.empty()) {
                std::cout << "ğŸš€ å¤„ç†ä¼˜å…ˆçº§æŸ¥è¯¢: \"" << query 
                          << "\" -> " << results.size() << " ä¸ªåŒ¹é…" << std::endl;
            }
        }
    }
    
    // æ™ºèƒ½é‡‡æ ·æŸ¥è¯¢ï¼ˆç”¨äºç¥ç»å…ƒè¾“å‡ºï¼‰
    bool smartSampleQuery(double neuron_activity, 
                         const std::string& neuron_output,
                         double similarity_threshold = 0.3) {
        
        if (sampling_controller.shouldSample(neuron_activity, neuron_output)) {
            // æ·»åŠ åˆ°ä¼˜å…ˆçº§é˜Ÿåˆ—
            asyncQuery(neuron_output, neuron_activity);
            return true;
        }
        return false;
    }
    
    // è·å–ç»Ÿè®¡ä¿¡æ¯
    struct QueryStats {
        SemanticQueryCache::CacheStats cache_stats;
        size_t queue_size;
        std::chrono::milliseconds current_sampling_interval;
        std::vector<std::string> recent_context;
    };
    
    QueryStats getStats() const {
        QueryStats stats;
        stats.cache_stats = cache.getStats();
        stats.queue_size = priority_manager.getQueueSize();
        stats.current_sampling_interval = sampling_controller.getCurrentInterval();
        stats.recent_context = context_processor.getContext();
        return stats;
    }
    
    // é‡ç½®çŠ¶æ€
    void reset() {
        cache.clearCache();
        context_processor.clearContext();
        sampling_controller.reset();
        priority_manager.clearQueue();
    }
    
    // é…ç½®æ¥å£
    void configure(size_t new_cache_size, 
                  size_t new_context_window,
                  double new_sampling_threshold,
                  std::chrono::milliseconds new_sampling_interval) {
        cache = SemanticQueryCache(new_cache_size);
        context_processor = ContextAwareQueryProcessor(cache, new_context_window);
        sampling_controller.setSamplingParameters(new_sampling_threshold, new_sampling_interval);
    }
};

#endif // SEMANTIC_QUERY_INTERFACE_H